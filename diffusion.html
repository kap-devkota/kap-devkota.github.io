<!DOCTYPE html>
<html>
<head>
<title> Flow matching for generative modelling </title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script>
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [['$', '$'], ['\\(', '\\)']], // enable $...$ for inline
      displayMath: [['$$', '$$'], ['\\[', '\\]']] // block math
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  };
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>
<body>

<h2> Song et al</h2>
<h3>Background</h3>


<h4>SCORE MATCHING WITH LANGEVIN DYNAMICS (SMLD)</h4>
<p>
  Given some original distribution \(p_{DATA}(x)\). We perturb this by applying a kernel \(p_\sigma(\tilde{x} | x)\). Call the resulting distribution \(p_\sigma(\tilde{x})\).
</p>

<p>
If we successively start applying the noise kernel with sds \(\sigma_1 < \sigma_2 < \ldots < \sigma_N\), where \(p_{\sigma_1} \approx p_{DATA}\) and \(p_{\sigma_N} \approx \mathcal{N}(0, \sigma_N^2)\), we seek  to obtain the original distribution \(p_{DATA}\) back from \(p_N\). This can be done by constructing a model \(s(\theta, \sigma_i)\) that, given a sample \(x\) and a noise level \(\sigma\), approximates the gradient of the log kernel (\(\nabla\log(p_\sigma (\tilde{x}|x))\)). So the objective function becomes
</p>

\begin{equation}
\theta^* = \underset{\theta}{\arg\min} \sum_{i=1}^N \sigma_i^2 \mathbb{E}_{p_{DATA}(x)} \mathbb{E}_{p_{\sigma_i}(\tilde{x} | x)} [\|s_\theta(\tilde{x}, \sigma_i) - \nabla_{\tilde{x}} \log p_{\sigma_i}(\tilde{x} | x)\|_2^2] \tag{1}\label{smld-theta}
\end{equation}

<p>
  After the optimal \(s_\theta\) is obtained (using \ref{smld-theta}), we do the MCMC Langevin dynamics to obtain samples for each \(p_{\sigma_i}\).
</p>

$$
x_i^m = x_i^{m-1} + \epsilon_i s_{\theta^*}(x_i^{m-1}, \sigma_i) + \sqrt{2\epsilon_i}z_i^m,
$$


<h2>https://arxiv.org/pdf/2006.11239 Equation 3</h2>
To prove:
\begin{align} 
  \mathbb{E}\left[-\log p_\theta(\mathbf{x}_0)\right] \leq \mathbb{E}_q\left[ -\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \right]
\end{align}
<br/>
<em>Proof</em>:
\begin{align}
\mathbb{E}\left[-\log p_\theta(\mathbf{x}_0)\right] &= \mathbb{E}\left[-\log \left(\int p_\theta(\mathbf{x}_0)p_\theta(\mathbf{x}_{1:T}|\mathbf{x}_0) d\mathbf{x}_{1:T}\right)\right] \\
&= \mathbb{E}\left[-\log \left(\int p_\theta(\mathbf{x}_{0:T})\frac{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} d\mathbf{x}_{1:T}\right)\right] \\
&\leq -\int_{\mathbf{x}_{1:T}} \mathbb{E} \left[ -\log \left(\frac{p_\theta(\bf{x}_{0:T})}{q(\bf{x}_{1:T}|\bf{x}_0)}\right)\right]q(\mathbf{x}_{1:T}|\mathbf{x}_0) d\bf{x}_{1:T} & \text{Jensen's Inequality}\\
& \leq \mathbb{E}_q\left[ -\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \right]
\end{align}


</body>
</html>
